<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE topic PUBLIC "-//OASIS//DTD DITA Topic//EN" "topic.dtd">
<topic id="topic_xgq_zhx_r4">
  <title>Release notes (beta)</title>
  <body>
    <p>This release note is for the 3.0.0 BETA release. It contains a list of known and fixed
      issues.</p>
    
    <section>
      <title>Known issues</title>
      <dl>
        <dlentry>
          <dt><xref href="http://www.couchbase.com/issues/browse/MB-11661" scope="external"
              format="html">MB-11661</xref></dt>
          <dd>mem_used in increasing and dropping in basic setup with 5 buckets</dd>
        </dlentry>
        <dlentry>
          <dt><xref href="http://www.couchbase.com/issues/browse/MB-11658" scope="external"
              format="html">MB-11658</xref></dt>
          <dd>Tombstone purge does not work due to misinterpretation of the <codeph>drop_deletes</codeph> flag.</dd>
        </dlentry>
        <dlentry>
          <dt><xref href="http://www.couchbase.com/issues/browse/MB-11643" scope="external"
              format="html">MB-11643</xref></dt>
          <dd>Incoming workload suffers when XDCR enabled</dd>
        </dlentry>
        <dlentry>
          <dt><xref href="http://www.couchbase.com/issues/browse/MB-11642" scope="external"
              format="html">MB-11642</xref></dt>
          <dd>Intra-replication falling behind under moderate-heavy workload
            <p>Under a moderate replication workload (20-30K sets per second), 
              the backlog becomes too large resulting in high memory usage.</p></dd>
        </dlentry>
        <dlentry>
          <dt><xref href="http://www.couchbase.com/issues/browse/MB-11630" scope="external"
              format="html">MB-11630</xref></dt>
          <dd>Rebalance in operation exited with reason {bulk_set_vbucket_state_failed}</dd>
        </dlentry>
        <dlentry>
          <dt><xref href="http://www.couchbase.com/issues/browse/MB-11629" scope="external"
              format="html">MB-11629</xref></dt>
          <dd>Memcached crashed during rebalance</dd>
        </dlentry>
        <dlentry>
          <dt><xref href="http://www.couchbase.com/issues/browse/MB-11608" scope="external"
              format="html">MB-11608</xref></dt>
          <dd>It is possible to prevent ns_server shutdown with some use of http (was: KV+XDCR
            System test: Cluster in unstable state after warmup- 2 nodes in pending state, memcached
            did not start on 1 node)</dd>
        </dlentry>
        <dlentry>
          <dt><xref href="http://www.couchbase.com/issues/browse/MB-11606" scope="external"
              format="html">MB-11606</xref></dt>
          <dd>Error displayed when node did not mark to collect</dd>
        </dlentry>
        <dlentry>
          <dt><xref href="http://www.couchbase.com/issues/browse/MB-11601" scope="external"
              format="html">MB-11601</xref></dt>
          <dd>KV-only system test. Rebalance hangs in swap rebalance due to memory bloating in one
            node</dd>
        </dlentry>
        <dlentry>
          <dt><xref href="http://www.couchbase.com/issues/browse/MB-11597" scope="external"
              format="html">MB-11597</xref></dt>
          <dd>KV+XDCR System test: Mutation replication rate for uni-xdcr is almost zero(900K items
            remaining) while another bi-xdcr to same cluster is ~10k ops/sec </dd>
        </dlentry>
        <dlentry>
          <dt><xref href="http://www.couchbase.com/issues/browse/MB-11568" scope="external"
              format="html">MB-11568</xref></dt>
          <dd>IP does not show the same in each step of cluster wide collect info</dd>
        </dlentry>
        <dlentry>
          <dt><xref href="http://www.couchbase.com/issues/browse/MB-11554" scope="external"
              format="html">MB-11554</xref></dt>
          <dd>Metadata inconsistent between source and destination clusters.
          <p>After failing over and rebalancing nodes on a destination cluster and 
            then replicating from the source cluster to destination cluster, 
          the document metadata is inconsistent between source and destination clusters. 
          This behavior occurs when the cbrecovery (1st) and rebalance (2nd) sequence is performed.</p></dd>
        </dlentry>
        <dlentry>
          <dt><xref href="http://www.couchbase.com/issues/browse/MB-11553" scope="external"
              format="html">MB-11553</xref></dt>
          <dd>Resident memory ratio drops to zero on mere updates of existing items of single node
            with no upr consumers at all</dd>
        </dlentry>
        <dlentry>
          <dt><xref href="http://www.couchbase.com/issues/browse/MB-11435" scope="external"
              format="html">MB-11435</xref></dt>
          <dd>1500-2000% CPU utilization by beam.smp on source nodes in XDCR scenarios (was ~500% in
            2.5.x)</dd>
        </dlentry>
        <dlentry>
          <dt><xref href="http://www.couchbase.com/issues/browse/MB-11434" scope="external"
              format="html">MB-11434</xref></dt>
          <dd>600-800% CPU consumption by memcached on the source cluster. <p>In XDCR scenarios, the
              CPU usage for memcached process is more than two times the usage in the previous
              release. This is due to increased scheduling overhead from the shared thread
              pool.</p><p>Workaround: Reduce the number of threads on systems that have more than 30
              cores.</p></dd>
        </dlentry>
        <dlentry>
          <dt><xref href="http://www.couchbase.com/issues/browse/MB-11405" scope="external"
              format="html">MB-11405</xref></dt>
          <dd>~2400% CPU consumption by memcached during ongoing workload with five (5) buckets.
              <p>In XDCR scenarios, the CPU usage for memcached process is more than two times the
              usage in the previous release. This is due to increased scheduling overhead from the
              shared thread pool. </p><p>Workaround: Reduce the number of threads on systems that
              have more than 30 cores.</p></dd>
        </dlentry>
        <dlentry>
          <dt><xref href="http://www.couchbase.com/issues/browse/MB-11383" scope="external"
              format="html">MB-11383</xref></dt>
          <dd>warmup_min_items_threshold setting is not honored correctly in 3.0 warmup</dd>
        </dlentry>
        <dlentry>
          <dt><xref href="http://www.couchbase.com/issues/browse/MB-11299" scope="external"
              format="html">MB-11299</xref></dt>
          <dd>UPR replica streams cannot send items from partial snapshots</dd>
        </dlentry>
      </dl>
    </section>
    
    <section><title>Fixed issues</title>

        <dl>
          <dlentry>
            <dt><xref href="http://www.couchbase.com/issues/browse/MB-11636" scope="external"
              format="html">MB-11636</xref></dt>
            <dd>Initial index build is four times (4x) slower than normal.</dd>
          </dlentry>
          
          <dlentry>
            <dt><xref href="http://www.couchbase.com/issues/browse/MB-11621" scope="external"
              format="html">MB-11621</xref></dt>
            <dd>Rebalance fails when adding node back after failover (delta recovery)</dd>
          </dlentry>
          
          <dlentry>
            <dt><xref href="http://www.couchbase.com/issues/browse/MB-11605" scope="external"
              format="html">MB-11605</xref></dt>
            <dd>XDCR replication (source to destination cluster) hangs when a hard OOM is hit. 
              
              <p>Uni-directional XDCR replication gets stuck if the source cluster hits hard
              out-of-memory (OOM) prior to the start of XDCR replication. 
              A hard OOM means that the bucket RAM quota has been exceeded. </p>
              <p>Workaround: It is recoverable by increasing the bucket RAM quota on the source cluster.</p></dd>
              
          </dlentry>
            
            <dlentry>
              <dt><xref href="http://www.couchbase.com/issues/browse/MB-11578" scope="external"
                format="html">MB-11578</xref></dt>
              <dd>Crash with Segmentation fault in couch_view_group_cleanup </dd>
            </dlentry>
 
          <dlentry>
            <dt><xref href="http://www.couchbase.com/issues/browse/MB-10252" scope="external"
              format="html">MB-10252</xref></dt>
            <dd>total_fragmentation_bytes metric is misleading. This should NOT be in a release
              note because it's internal!</dd>
          </dlentry>
          
        </dl>
    
    
    
    </section>
  </body>
</topic>
