<?xml version="1.0" encoding="utf-8"?><?workdir /Users/Ruth/forks/docs-ng/dita/src/ag3/temp/pdf/ag3/Tasks?><?workdir-uri file:/Users/Ruth/forks/docs-ng/dita/src/ag3/temp/pdf/ag3/Tasks/?><?path2project ../?><topic xmlns:ditaarch="http://dita.oasis-open.org/architecture/2005/" xml:lang="en-us" id="topic17798" ditaarch:DITAArchVersion="1.2" domains="(topic hi-d)                             (topic ut-d)                             (topic indexing-d)                            (topic hazard-d)                            (topic abbrev-d)                            (topic pr-d)                             (topic sw-d)                            (topic ui-d)    " class="- topic/topic " xtrf="/Users/Ruth/forks/docs-ng/dita/src/ag3/Tasks/tasks-nodeFailover.dita" xtrc="topic:1;4:41">
  <title class="- topic/title " xtrf="/Users/Ruth/forks/docs-ng/dita/src/ag3/Tasks/tasks-nodeFailover.dita" xtrc="title:1;5:10">Failing over nodes</title>
  <shortdesc class="- topic/shortdesc " xtrf="/Users/Ruth/forks/docs-ng/dita/src/ag3/Tasks/tasks-nodeFailover.dita" xtrc="shortdesc:1;6:14">Failing over a node means that Couchbase Server removes the node from a cluster and
    makes replicated data at other nodes available for client requests.</shortdesc>
  <body class="- topic/body " xtrf="/Users/Ruth/forks/docs-ng/dita/src/ag3/Tasks/tasks-nodeFailover.dita" xtrc="body:1;8:9">
    <p class="- topic/p " xtrf="/Users/Ruth/forks/docs-ng/dita/src/ag3/Tasks/tasks-nodeFailover.dita" xtrc="p:1;9:8"> Because Couchbase Server provides data replication within a cluster, the cluster can handle
      failure of one or more nodes without affecting your ability to access the stored data. In the
      event of a node failure, you can manually initiate a <codeph class="+ topic/ph pr-d/codeph " xtrf="/Users/Ruth/forks/docs-ng/dita/src/ag3/Tasks/tasks-nodeFailover.dita" xtrc="codeph:1;11:68">failover</codeph> status for the
      node in Web Console and resolve the issues.</p>
    <p class="- topic/p " xtrf="/Users/Ruth/forks/docs-ng/dita/src/ag3/Tasks/tasks-nodeFailover.dita" xtrc="p:2;13:8">Alternately you can configure Couchbase Server so it will <i class="+ topic/ph hi-d/i " xtrf="/Users/Ruth/forks/docs-ng/dita/src/ag3/Tasks/tasks-nodeFailover.dita" xtrc="i:1;13:69">automatically</i> remove a
      failed node from a cluster and have the cluster operate in a degraded mode. If you choose this
      automatic option, the workload for functioning nodes that remain the cluster will increase.
      You will still need to address the node failure, return a functioning node to the cluster and
      then rebalance the cluster in order for the cluster to function as it did prior to node
      failure.</p>
    <p class="- topic/p " xtrf="/Users/Ruth/forks/docs-ng/dita/src/ag3/Tasks/tasks-nodeFailover.dita" xtrc="p:3;19:8">Whether you manually failover a node or have Couchbase Server perform automatic failover, you
      should determine the underlying cause for the failure. You should then set up functioning
      nodes, add the nodes, and then rebalance the cluster. Keep in mind the following guidelines on
      replacing or adding nodes when you cope with node failure and failover scenarios:</p>
    <ul class="- topic/ul " xtrf="/Users/Ruth/forks/docs-ng/dita/src/ag3/Tasks/tasks-nodeFailover.dita" xtrc="ul:1;23:9">
      <li class="- topic/li " xtrf="/Users/Ruth/forks/docs-ng/dita/src/ag3/Tasks/tasks-nodeFailover.dita" xtrc="li:1;24:11"><p class="- topic/p " xtrf="/Users/Ruth/forks/docs-ng/dita/src/ag3/Tasks/tasks-nodeFailover.dita" xtrc="p:4;24:14">If the node failed due to a hardware or system failure, you should add a new
          replacement node to the cluster and rebalance.</p></li>
      <li class="- topic/li " xtrf="/Users/Ruth/forks/docs-ng/dita/src/ag3/Tasks/tasks-nodeFailover.dita" xtrc="li:2;26:11"><p class="- topic/p " xtrf="/Users/Ruth/forks/docs-ng/dita/src/ag3/Tasks/tasks-nodeFailover.dita" xtrc="p:5;26:14">If the node failed because of capacity problems in your cluster, you should replace the
          node but also add additional nodes to meet the capacity needs.</p></li>
      <li class="- topic/li " xtrf="/Users/Ruth/forks/docs-ng/dita/src/ag3/Tasks/tasks-nodeFailover.dita" xtrc="li:3;28:11"><p class="- topic/p " xtrf="/Users/Ruth/forks/docs-ng/dita/src/ag3/Tasks/tasks-nodeFailover.dita" xtrc="p:6;28:14">If the node failure was transient in nature and the failed node functions once again,
          you can add the node back to the cluster.</p></li>
    </ul>
    <p class="- topic/p " xtrf="/Users/Ruth/forks/docs-ng/dita/src/ag3/Tasks/tasks-nodeFailover.dita" xtrc="p:7;31:8">Be aware that failover is a distinct operation compared to <i class="+ topic/ph hi-d/i " xtrf="/Users/Ruth/forks/docs-ng/dita/src/ag3/Tasks/tasks-nodeFailover.dita" xtrc="i:2;31:70">removing/rebalancing</i> a
      node. Typically you remove a <i class="+ topic/ph hi-d/i " xtrf="/Users/Ruth/forks/docs-ng/dita/src/ag3/Tasks/tasks-nodeFailover.dita" xtrc="i:3;32:39">functioning node</i> from a cluster for maintenance, or other
      reasons; in contrast you perform a failover for a node that does not function.</p>
    <p class="- topic/p " xtrf="/Users/Ruth/forks/docs-ng/dita/src/ag3/Tasks/tasks-nodeFailover.dita" xtrc="p:8;34:8">When you remove a functioning node from a cluster, you use Web Console to indicate the node
      will be removed, then you rebalance the cluster so that data requests for the node can be
      handled by other nodes. Since the node you want to remove still functions, it is able to
      handle data requests until the rebalance completes. At this point, other nodes in the cluster
      will handle data requests. There is therefore no disruption in data service or no loss of data
      that can occur when you remove a node then rebalance the cluster. If you need to remove a
      functioning node for administration purposes, you should use the remove and rebalance
      functionality not failover.</p>
    <p class="- topic/p " xtrf="/Users/Ruth/forks/docs-ng/dita/src/ag3/Tasks/tasks-nodeFailover.dita" xtrc="p:9;42:8">If you try to failover a functioning node it may result in data loss. This is because
      failover will immediately remove the node from the cluster and any data that has not yet been
      replicated to other nodes may be permanently lost if it had not been persisted to disk.</p>
    <p class="- topic/p " xtrf="/Users/Ruth/forks/docs-ng/dita/src/ag3/Tasks/tasks-nodeFailover.dita" xtrc="p:10;45:8">For more information about performing failover see the following resources:</p>
    <ul class="- topic/ul " xtrf="/Users/Ruth/forks/docs-ng/dita/src/ag3/Tasks/tasks-nodeFailover.dita" xtrc="ul:2;46:9">
      <li class="- topic/li " xtrf="/Users/Ruth/forks/docs-ng/dita/src/ag3/Tasks/tasks-nodeFailover.dita" xtrc="li:4;47:11"><b class="+ topic/ph hi-d/b " xtrf="/Users/Ruth/forks/docs-ng/dita/src/ag3/Tasks/tasks-nodeFailover.dita" xtrc="b:1;47:14">Automated failover</b> will automatically mark a node as failed over if the node has
        been identified as unresponsive or unavailable. There are some deliberate limitations to the
        automated failover feature. </li>
    </ul>
    <ul class="- topic/ul " xtrf="/Users/Ruth/forks/docs-ng/dita/src/ag3/Tasks/tasks-nodeFailover.dita" xtrc="ul:3;51:9">
      <li class="- topic/li " xtrf="/Users/Ruth/forks/docs-ng/dita/src/ag3/Tasks/tasks-nodeFailover.dita" xtrc="li:5;52:11"><p class="- topic/p " xtrf="/Users/Ruth/forks/docs-ng/dita/src/ag3/Tasks/tasks-nodeFailover.dita" xtrc="p:11;52:14"><b class="+ topic/ph hi-d/b " xtrf="/Users/Ruth/forks/docs-ng/dita/src/ag3/Tasks/tasks-nodeFailover.dita" xtrc="b:2;52:17">Initiating a failover</b> whether or not you use automatic or manual failover, you
          need to perform additional steps to bring a cluster into a fully functioning
        state.</p></li>
      <li class="- topic/li " xtrf="/Users/Ruth/forks/docs-ng/dita/src/ag3/Tasks/tasks-nodeFailover.dita" xtrc="li:6;55:11"><p class="- topic/p " xtrf="/Users/Ruth/forks/docs-ng/dita/src/ag3/Tasks/tasks-nodeFailover.dita" xtrc="p:12;55:14"><b class="+ topic/ph hi-d/b " xtrf="/Users/Ruth/forks/docs-ng/dita/src/ag3/Tasks/tasks-nodeFailover.dita" xtrc="b:3;55:17">Adding nodes after failover</b>. After you resolve the issue with the failed over
          node you can add the node back to your cluster. </p></li>
    </ul>
    
    
    <section class="- topic/section " xtrf="/Users/Ruth/forks/docs-ng/dita/src/ag3/Tasks/tasks-nodeFailover.dita" xtrc="section:1;60:14"><title class="- topic/title " xtrf="/Users/Ruth/forks/docs-ng/dita/src/ag3/Tasks/tasks-nodeFailover.dita" xtrc="title:2;60:21">Choosing a failover solution</title>
      <p class="- topic/p " xtrf="/Users/Ruth/forks/docs-ng/dita/src/ag3/Tasks/tasks-nodeFailover.dita" xtrc="p:13;61:10">Because node failover has the potential
        to reduce the performance of your cluster, you should consider how best to handle a failover
        situation. Using automated failover means that a cluster can fail over a node without
        user-intervention and without knowledge and identification of the issue that caused the node
        failure. It still requires you to initiate a rebalance in order to return the cluster to a
        healthy state.</p><p class="- topic/p " xtrf="/Users/Ruth/forks/docs-ng/dita/src/ag3/Tasks/tasks-nodeFailover.dita" xtrc="p:14;66:30">If you choose manual failover to manage your cluster you need to
        monitor the cluster and identify when an issue occurs. If an issues does occur you then
        trigger a manual failover and rebalance operation. This approach requires more monitoring
        and manual intervention, there is also still a possibility that your cluster and data access
        may still degrade before you initiate failover and rebalance.</p><p class="- topic/p " xtrf="/Users/Ruth/forks/docs-ng/dita/src/ag3/Tasks/tasks-nodeFailover.dita" xtrc="p:15;70:77">In the following
        sections the two alternatives and their issues are described in more
          detail.</p><p class="- topic/p " xtrf="/Users/Ruth/forks/docs-ng/dita/src/ag3/Tasks/tasks-nodeFailover.dita" xtrc="p:16;72:25"><b class="+ topic/ph hi-d/b " xtrf="/Users/Ruth/forks/docs-ng/dita/src/ag3/Tasks/tasks-nodeFailover.dita" xtrc="b:4;72:28">Automated failover considerations</b></p><p class="- topic/p " xtrf="/Users/Ruth/forks/docs-ng/dita/src/ag3/Tasks/tasks-nodeFailover.dita" xtrc="p:17;72:72">Automatically failing
        components in any distributed system can cause problems. If you cannot identify the cause of
        failure, and you do not understand the load that will be placed on the remaining system,
        then automated failover can cause more problems than it is designed to solve. Some of the
        situations that might lead to problems include:</p>
      
      <ul class="- topic/ul " xtrf="/Users/Ruth/forks/docs-ng/dita/src/ag3/Tasks/tasks-nodeFailover.dita" xtrc="ul:4;78:11">
        <li class="- topic/li " xtrf="/Users/Ruth/forks/docs-ng/dita/src/ag3/Tasks/tasks-nodeFailover.dita" xtrc="li:7;79:13"><b class="+ topic/ph hi-d/b " xtrf="/Users/Ruth/forks/docs-ng/dita/src/ag3/Tasks/tasks-nodeFailover.dita" xtrc="b:5;79:16">Avoiding failover chain-reactions (Thundering herd)</b></li>
      </ul><p class="- topic/p " xtrf="/Users/Ruth/forks/docs-ng/dita/src/ag3/Tasks/tasks-nodeFailover.dita" xtrc="p:18;80:15">Imagine a scenario where a Couchbase Server cluster of five nodes is operating at
        80–90% aggregate capacity in terms of network load. Everything is running well but at the
        limit of cluster capacity. Imagine a node fails and the software decides to automatically
        failover that node. It is unlikely that all of the remaining four nodes are be able to
        successfully handle the additional load.</p><p class="- topic/p " xtrf="/Users/Ruth/forks/docs-ng/dita/src/ag3/Tasks/tasks-nodeFailover.dita" xtrc="p:19;84:56">The result is that the increased load could
        lead to another node failing and being automatically failed over. These failures can cascade
        and lead to the eventual loss of an entire cluster. Clearly having 1/5th of the requests not
        being serviced due to single node failure would be more desirable than none of the requests
        being serviced due to an entire cluster failure.</p><p class="- topic/p " xtrf="/Users/Ruth/forks/docs-ng/dita/src/ag3/Tasks/tasks-nodeFailover.dita" xtrc="p:20;88:64">The solution in this case is to
        continue cluster operations with the single node failure, add a new server to the cluster to
        handle the missing capacity, mark the failed node for removal and then rebalance. This way
        there is a brief partial outage rather than an entire cluster being disabled.</p><p class="- topic/p " xtrf="/Users/Ruth/forks/docs-ng/dita/src/ag3/Tasks/tasks-nodeFailover.dita" xtrc="p:21;91:93">One
        alternate preventative solution is to ensure there is excess capacity to handle unexpected
        node failures and allow replicas to take over.</p>
      
      <ul class="- topic/ul " xtrf="/Users/Ruth/forks/docs-ng/dita/src/ag3/Tasks/tasks-nodeFailover.dita" xtrc="ul:5;95:11">
        <li class="- topic/li " xtrf="/Users/Ruth/forks/docs-ng/dita/src/ag3/Tasks/tasks-nodeFailover.dita" xtrc="li:8;96:13"><b class="+ topic/ph hi-d/b " xtrf="/Users/Ruth/forks/docs-ng/dita/src/ag3/Tasks/tasks-nodeFailover.dita" xtrc="b:6;96:16">Handling failovers with network partitions</b></li>
      </ul><p class="- topic/p " xtrf="/Users/Ruth/forks/docs-ng/dita/src/ag3/Tasks/tasks-nodeFailover.dita" xtrc="p:22;97:15">In case of network partition or split-brain where the failure of a network device
        causes a network to be split, Couchbase implements automatic failover with the following
        restrictions:</p><ul class="- topic/ul " xtrf="/Users/Ruth/forks/docs-ng/dita/src/ag3/Tasks/tasks-nodeFailover.dita" xtrc="ul:6;99:30">
        <li class="- topic/li " xtrf="/Users/Ruth/forks/docs-ng/dita/src/ag3/Tasks/tasks-nodeFailover.dita" xtrc="li:9;100:13">Automatic failover requires a minimum of three (3) nodes per cluster. This prevents a
          2-node cluster from having both nodes fail each other over in the face of a network
          partition and protects the data integrity and consistency.</li>
        <li class="- topic/li " xtrf="/Users/Ruth/forks/docs-ng/dita/src/ag3/Tasks/tasks-nodeFailover.dita" xtrc="li:10;103:13">Automatic failover occurs only if exactly one (1) node is down. This prevents a network
          partition from causing two or more halves of a cluster from failing each other over and
          protects the data integrity and consistency.</li>
        <li class="- topic/li " xtrf="/Users/Ruth/forks/docs-ng/dita/src/ag3/Tasks/tasks-nodeFailover.dita" xtrc="li:11;106:13">Automatic failover occurs only once before requiring administrative action. This
          prevents cascading failovers and subsequent performance and stability degradation. In many
          cases, it is better to not have access to a small part of the dataset rather than having a
          cluster continuously degrade itself to the point of being non-functional.</li>
        <li class="- topic/li " xtrf="/Users/Ruth/forks/docs-ng/dita/src/ag3/Tasks/tasks-nodeFailover.dita" xtrc="li:12;110:13">Automatic failover implements a 30 second delay when a node fails before it performs an
          automatic failover. This prevents transient network issues or slowness from causing a node
          to be failed over when it shouldn’t be.</li>
      </ul><p class="- topic/p " xtrf="/Users/Ruth/forks/docs-ng/dita/src/ag3/Tasks/tasks-nodeFailover.dita" xtrc="p:23;113:15">If a network partition occurs, automatic failover occurs if and only if automatic
        failover is allowed by the specified restrictions. For example, if a single node is
        partitioned out of a cluster of five (5), it is automatically failed over. If more than one
        (1) node is partitioned off, autofailover does not occur. After that, administrative action
        is required for a reset. In the event that another node fails before the automatic failover
        is reset, no automatic failover occurs. </p><ul class="- topic/ul " xtrf="/Users/Ruth/forks/docs-ng/dita/src/ag3/Tasks/tasks-nodeFailover.dita" xtrc="ul:7;118:57">
        <li class="- topic/li " xtrf="/Users/Ruth/forks/docs-ng/dita/src/ag3/Tasks/tasks-nodeFailover.dita" xtrc="li:13;119:13"><b class="+ topic/ph hi-d/b " xtrf="/Users/Ruth/forks/docs-ng/dita/src/ag3/Tasks/tasks-nodeFailover.dita" xtrc="b:7;119:16">Handling misbehaving nodes</b></li>
      </ul><p class="- topic/p " xtrf="/Users/Ruth/forks/docs-ng/dita/src/ag3/Tasks/tasks-nodeFailover.dita" xtrc="p:24;120:15">There are cases where one node loses connectivity to the cluster or functions as if it
        has lost connectivity to the cluster. If you enable it to automatically failover the rest of
        the cluster, that node is able to create a cluster-of-one. The result for your cluster is a
        similar partition situation we described previously.</p><p class="- topic/p " xtrf="/Users/Ruth/forks/docs-ng/dita/src/ag3/Tasks/tasks-nodeFailover.dita" xtrc="p:25;123:68">In this case you should make sure
        there is spare node capacity in your cluster and failover the node with network issues. If
        you determine there is not enough capacity, add a node to handle the capacity after your
        failover the node with issues.</p><p class="- topic/p " xtrf="/Users/Ruth/forks/docs-ng/dita/src/ag3/Tasks/tasks-nodeFailover.dita" xtrc="p:26;126:46"><b class="+ topic/ph hi-d/b " xtrf="/Users/Ruth/forks/docs-ng/dita/src/ag3/Tasks/tasks-nodeFailover.dita" xtrc="b:8;126:49">Manual or monitored failover</b></p><p class="- topic/p " xtrf="/Users/Ruth/forks/docs-ng/dita/src/ag3/Tasks/tasks-nodeFailover.dita" xtrc="p:27;126:88">Performing
        manual failover through monitoring can take two forms, either by human monitoring or by
        using a system external to the Couchbase Server cluster. An external monitoring system can
        monitor both the cluster and the node environment and make a more information-driven
        decision. If you choose a manual failover solution, there are also issues you should be
        aware of. Although automated failover has potential issues, choosing to use manual or
        monitored failover is not without potential problems.</p><ul class="- topic/ul " xtrf="/Users/Ruth/forks/docs-ng/dita/src/ag3/Tasks/tasks-nodeFailover.dita" xtrc="ul:8;132:70">
        <li class="- topic/li " xtrf="/Users/Ruth/forks/docs-ng/dita/src/ag3/Tasks/tasks-nodeFailover.dita" xtrc="li:14;133:13"><b class="+ topic/ph hi-d/b " xtrf="/Users/Ruth/forks/docs-ng/dita/src/ag3/Tasks/tasks-nodeFailover.dita" xtrc="b:9;133:16">Human intervention</b></li>
      </ul><p class="- topic/p " xtrf="/Users/Ruth/forks/docs-ng/dita/src/ag3/Tasks/tasks-nodeFailover.dita" xtrc="p:28;134:15">One option is to have a human operator respond to alerts and make a decision on what
        to do. Humans are uniquely capable of considering a wide range of data, observations and
        experiences to best resolve a situation. Many organizations disallow automated failover
        without human consideration of the implications. The drawback of using human intervention is
        that it will be slower to respond than using a computer-based monitoring system.</p><ul class="- topic/ul " xtrf="/Users/Ruth/forks/docs-ng/dita/src/ag3/Tasks/tasks-nodeFailover.dita" xtrc="ul:9;138:97">
        <li class="- topic/li " xtrf="/Users/Ruth/forks/docs-ng/dita/src/ag3/Tasks/tasks-nodeFailover.dita" xtrc="li:15;139:13"><b class="+ topic/ph hi-d/b " xtrf="/Users/Ruth/forks/docs-ng/dita/src/ag3/Tasks/tasks-nodeFailover.dita" xtrc="b:10;139:16">External monitoring</b></li>
      </ul><p class="- topic/p " xtrf="/Users/Ruth/forks/docs-ng/dita/src/ag3/Tasks/tasks-nodeFailover.dita" xtrc="p:29;140:15">Another option is to have a system monitoring the cluster via the Couchbase REST API.
        Such an external system is in a good position to failover nodes because it can take into
        account system components that are outside the scope of Couchbase Server.</p><p class="- topic/p " xtrf="/Users/Ruth/forks/docs-ng/dita/src/ag3/Tasks/tasks-nodeFailover.dita" xtrc="p:30;142:89">For example
        monitoring software can observe that a network switch is failing and that there is a
        dependency on that switch by the Couchbase cluster. The system can determine that failing
        Couchbase Server nodes will not help the situation and will therefore not failover the
        node.</p><p class="- topic/p " xtrf="/Users/Ruth/forks/docs-ng/dita/src/ag3/Tasks/tasks-nodeFailover.dita" xtrc="p:31;146:21">The monitoring system can also determine that components around Couchbase Server
        are functioning and that various nodes in the cluster are healthy. If the monitoring system
        determines the problem is only with a single node and remaining nodes in the cluster can
        support aggregate traffic, then the system may failover the node using the REST API or
        command-line tools.</p></section>

    <section class="- topic/section " xtrf="/Users/Ruth/forks/docs-ng/dita/src/ag3/Tasks/tasks-nodeFailover.dita" xtrc="section:2;152:14"><title class="- topic/title " xtrf="/Users/Ruth/forks/docs-ng/dita/src/ag3/Tasks/tasks-nodeFailover.dita" xtrc="title:3;152:21">Using automatic failover</title><p class="- topic/p " xtrf="/Users/Ruth/forks/docs-ng/dita/src/ag3/Tasks/tasks-nodeFailover.dita" xtrc="p:32;152:56">There are a number of restrictions on
        automatic failover in Couchbase Server. This is to help prevent some issues that can occur
        when you use automatic failover. </p><ul class="- topic/ul " xtrf="/Users/Ruth/forks/docs-ng/dita/src/ag3/Tasks/tasks-nodeFailover.dita" xtrc="ul:10;154:50">
        <li class="- topic/li " xtrf="/Users/Ruth/forks/docs-ng/dita/src/ag3/Tasks/tasks-nodeFailover.dita" xtrc="li:16;155:13"><p class="- topic/p " xtrf="/Users/Ruth/forks/docs-ng/dita/src/ag3/Tasks/tasks-nodeFailover.dita" xtrc="p:33;155:16"><b class="+ topic/ph hi-d/b " xtrf="/Users/Ruth/forks/docs-ng/dita/src/ag3/Tasks/tasks-nodeFailover.dita" xtrc="b:11;155:19">Disabled by Default</b> Automatic failover is disabled by default. This prevents
            Couchbase Server from using automatic failover without you explicitly enabling
          it.</p></li>
        <li class="- topic/li " xtrf="/Users/Ruth/forks/docs-ng/dita/src/ag3/Tasks/tasks-nodeFailover.dita" xtrc="li:17;158:13"><p class="- topic/p " xtrf="/Users/Ruth/forks/docs-ng/dita/src/ag3/Tasks/tasks-nodeFailover.dita" xtrc="p:34;158:16"><b class="+ topic/ph hi-d/b " xtrf="/Users/Ruth/forks/docs-ng/dita/src/ag3/Tasks/tasks-nodeFailover.dita" xtrc="b:12;158:19">Minimum Nodes</b> Automatic failover is only available on clusters of at least
            three nodes.</p></li>
      </ul><p class="- topic/p " xtrf="/Users/Ruth/forks/docs-ng/dita/src/ag3/Tasks/tasks-nodeFailover.dita" xtrc="p:35;160:15">If two or more nodes go down at the same time within a specified delay period, the
        automatic failover system will not failover any nodes.</p><ul class="- topic/ul " xtrf="/Users/Ruth/forks/docs-ng/dita/src/ag3/Tasks/tasks-nodeFailover.dita" xtrc="ul:11;161:71">
        <li class="- topic/li " xtrf="/Users/Ruth/forks/docs-ng/dita/src/ag3/Tasks/tasks-nodeFailover.dita" xtrc="li:18;162:13"><p class="- topic/p " xtrf="/Users/Ruth/forks/docs-ng/dita/src/ag3/Tasks/tasks-nodeFailover.dita" xtrc="p:36;162:16"><b class="+ topic/ph hi-d/b " xtrf="/Users/Ruth/forks/docs-ng/dita/src/ag3/Tasks/tasks-nodeFailover.dita" xtrc="b:13;162:19">Required Intervention</b> Automatic failover will only fail over one node before
            requiring human intervention. This is to prevent a chain reaction failure of all nodes
            in the cluster.</p></li>
        <li class="- topic/li " xtrf="/Users/Ruth/forks/docs-ng/dita/src/ag3/Tasks/tasks-nodeFailover.dita" xtrc="li:19;165:13"><p class="- topic/p " xtrf="/Users/Ruth/forks/docs-ng/dita/src/ag3/Tasks/tasks-nodeFailover.dita" xtrc="p:37;165:16"><b class="+ topic/ph hi-d/b " xtrf="/Users/Ruth/forks/docs-ng/dita/src/ag3/Tasks/tasks-nodeFailover.dita" xtrc="b:14;165:19">Failover Delay</b> There is a minimum 30 second delay before a node will be failed
            over. This time can be raised, but the software is hard coded to perform multiple pings
            of a node that may be down. This is to prevent failover of a functioning but slow node
            or to prevent network connection issues from triggering failover. </p></li>
      </ul><p class="- topic/p " xtrf="/Users/Ruth/forks/docs-ng/dita/src/ag3/Tasks/tasks-nodeFailover.dita" xtrc="p:38;169:15">You can use the REST API to configure an email notification that will be sent by
        Couchbase Server if any node failures occur and node is automatically failed over. </p>
      <p class="- topic/p " xtrf="/Users/Ruth/forks/docs-ng/dita/src/ag3/Tasks/tasks-nodeFailover.dita" xtrc="p:39;171:10">Once an automatic failover has occurred, the Couchbase Cluster is relying on other nodes to
        serve replicated data. You should initiate a rebalance to return your cluster to a fully
        functioning state. </p>
      <p class="- topic/p " xtrf="/Users/Ruth/forks/docs-ng/dita/src/ag3/Tasks/tasks-nodeFailover.dita" xtrc="p:40;174:10"><b class="+ topic/ph hi-d/b " xtrf="/Users/Ruth/forks/docs-ng/dita/src/ag3/Tasks/tasks-nodeFailover.dita" xtrc="b:15;174:13">Resetting the Automatic failover counter</b></p><p class="- topic/p " xtrf="/Users/Ruth/forks/docs-ng/dita/src/ag3/Tasks/tasks-nodeFailover.dita" xtrc="p:41;174:64">After a node has been automatically
        failed over, Couchbase Server increments an internal counter that indicates if a node has
        been failed over. This counter prevents the server from automatically failing over
        additional nodes until you identify the issue that caused the failover and resolve it. If
        the internal counter indicates a node has failed over, the server will no longer
        automatically failover additional nodes in the cluster. You will need to re-enable automatic
        failover in a cluster by resetting this counter.</p><p class="- topic/p " xtrf="/Users/Ruth/forks/docs-ng/dita/src/ag3/Tasks/tasks-nodeFailover.dita" xtrc="p:42;180:64">Important</p><p class="- topic/p " xtrf="/Users/Ruth/forks/docs-ng/dita/src/ag3/Tasks/tasks-nodeFailover.dita" xtrc="p:43;180:80">Reset the automatic
        failover only after the node issue is resolved, rebalance occurs, and the cluster is
        restored to a fully functioning state.</p><p class="- topic/p " xtrf="/Users/Ruth/forks/docs-ng/dita/src/ag3/Tasks/tasks-nodeFailover.dita" xtrc="p:44;182:54">You can reset the counter using the REST
        API:</p><codeblock xml:space="preserve" class="+ topic/pre pr-d/codeblock " xtrf="/Users/Ruth/forks/docs-ng/dita/src/ag3/Tasks/tasks-nodeFailover.dita" xtrc="codeblock:1;183:28"><codeph class="+ topic/ph pr-d/codeph " xtrf="/Users/Ruth/forks/docs-ng/dita/src/ag3/Tasks/tasks-nodeFailover.dita" xtrc="codeph:2;183:36">&gt; curl -i -u cluster-username:cluster-password \
    http://localhost:8091/settings/autoFailover/resetCount
</codeph></codeblock>
    </section>
    <section class="- topic/section " xtrf="/Users/Ruth/forks/docs-ng/dita/src/ag3/Tasks/tasks-nodeFailover.dita" xtrc="section:3;187:14"><title class="- topic/title " xtrf="/Users/Ruth/forks/docs-ng/dita/src/ag3/Tasks/tasks-nodeFailover.dita" xtrc="title:4;187:21">Initiating a node failover</title><p class="- topic/p " xtrf="/Users/Ruth/forks/docs-ng/dita/src/ag3/Tasks/tasks-nodeFailover.dita" xtrc="p:45;187:58">If you need to remove a node from the
        cluster due to hardware or system failure, you need to indicate the failover status for that
        node. This causes Couchbase Server to use replicated data from other functioning nodes in
        the cluster.</p><p class="- topic/p " xtrf="/Users/Ruth/forks/docs-ng/dita/src/ag3/Tasks/tasks-nodeFailover.dita" xtrc="p:46;190:28">Important</p><p class="- topic/p " xtrf="/Users/Ruth/forks/docs-ng/dita/src/ag3/Tasks/tasks-nodeFailover.dita" xtrc="p:47;190:44">Before you indicate the failover for a node, read
        [Failing Over Nodes](#couchbase-admin-tasks-failover). Do not use failover to remove a
        functioning node from the cluster for administration or upgrade. This is because initiating
        a failover for a node activates replicated data at other nodes which reduces the overall
        capacity of the cluster. Data from the failover node that has not yet been replicated at
        other nodes or persisted on disk will be lost. For information about removing and adding a
        node, see [Performing a Rebalance, Adding a Node to a
        Cluster](../cb-admin/#couchbase-admin-tasks-addremove-rebalance). </p><p class="- topic/p " xtrf="/Users/Ruth/forks/docs-ng/dita/src/ag3/Tasks/tasks-nodeFailover.dita" xtrc="p:48;197:82">You can provide the
        failover status for a node with two different methods:</p><ul class="- topic/ul " xtrf="/Users/Ruth/forks/docs-ng/dita/src/ag3/Tasks/tasks-nodeFailover.dita" xtrc="ul:12;198:71">
        <li class="- topic/li " xtrf="/Users/Ruth/forks/docs-ng/dita/src/ag3/Tasks/tasks-nodeFailover.dita" xtrc="li:20;199:13"><b class="+ topic/ph hi-d/b " xtrf="/Users/Ruth/forks/docs-ng/dita/src/ag3/Tasks/tasks-nodeFailover.dita" xtrc="b:16;199:16">Using the Web Console</b></li>
      </ul><p class="- topic/p " xtrf="/Users/Ruth/forks/docs-ng/dita/src/ag3/Tasks/tasks-nodeFailover.dita" xtrc="p:49;200:15">Go to the <codeph class="+ topic/ph pr-d/codeph " xtrf="/Users/Ruth/forks/docs-ng/dita/src/ag3/Tasks/tasks-nodeFailover.dita" xtrc="codeph:3;200:33">Management -&gt; Server Nodes</codeph> section of the Web Console.
        Find the node that you want to failover, and click the <codeph class="+ topic/ph pr-d/codeph " xtrf="/Users/Ruth/forks/docs-ng/dita/src/ag3/Tasks/tasks-nodeFailover.dita" xtrc="codeph:4;201:72">Fail Over</codeph> button.
        You can only failover nodes that the cluster has identified as being Down.</p><p class="- topic/p " xtrf="/Users/Ruth/forks/docs-ng/dita/src/ag3/Tasks/tasks-nodeFailover.dita" xtrc="p:50;202:90">Web Console
        will display a warning message.</p><p class="- topic/p " xtrf="/Users/Ruth/forks/docs-ng/dita/src/ag3/Tasks/tasks-nodeFailover.dita" xtrc="p:51;203:47">Click <codeph class="+ topic/ph pr-d/codeph " xtrf="/Users/Ruth/forks/docs-ng/dita/src/ag3/Tasks/tasks-nodeFailover.dita" xtrc="codeph:5;203:61">Fail Over</codeph> to indicate the node
        is failed over. You can also choose to <codeph class="+ topic/ph pr-d/codeph " xtrf="/Users/Ruth/forks/docs-ng/dita/src/ag3/Tasks/tasks-nodeFailover.dita" xtrc="codeph:6;204:56">Cancel</codeph>.</p><ul class="- topic/ul " xtrf="/Users/Ruth/forks/docs-ng/dita/src/ag3/Tasks/tasks-nodeFailover.dita" xtrc="ul:13;204:80">
        <li class="- topic/li " xtrf="/Users/Ruth/forks/docs-ng/dita/src/ag3/Tasks/tasks-nodeFailover.dita" xtrc="li:21;205:13"><b class="+ topic/ph hi-d/b " xtrf="/Users/Ruth/forks/docs-ng/dita/src/ag3/Tasks/tasks-nodeFailover.dita" xtrc="b:17;205:16">Using the Command-line</b></li>
      </ul><p class="- topic/p " xtrf="/Users/Ruth/forks/docs-ng/dita/src/ag3/Tasks/tasks-nodeFailover.dita" xtrc="p:52;206:15">You can failover one or more nodes using the <codeph class="+ topic/ph pr-d/codeph " xtrf="/Users/Ruth/forks/docs-ng/dita/src/ag3/Tasks/tasks-nodeFailover.dita" xtrc="codeph:7;206:68">failover</codeph> command in
          <codeph class="+ topic/ph pr-d/codeph " xtrf="/Users/Ruth/forks/docs-ng/dita/src/ag3/Tasks/tasks-nodeFailover.dita" xtrc="codeph:8;207:19">couchbase-cli</codeph>. To failover the node, you must specify the IP address and
        port, if not the standard port for the node you want to failover. For
        example:</p><codeblock xml:space="preserve" class="+ topic/pre pr-d/codeblock " xtrf="/Users/Ruth/forks/docs-ng/dita/src/ag3/Tasks/tasks-nodeFailover.dita" xtrc="codeblock:2;209:32"><codeph class="+ topic/ph pr-d/codeph " xtrf="/Users/Ruth/forks/docs-ng/dita/src/ag3/Tasks/tasks-nodeFailover.dita" xtrc="codeph:9;209:40">```
&gt; ﻿couchbase-cli failover --cluster=localhost:8091\
    -u cluster-username -p cluster-password\
    --server-failover=192.168.0.72:8091
```
</codeph></codeblock><p class="- topic/p " xtrf="/Users/Ruth/forks/docs-ng/dita/src/ag3/Tasks/tasks-nodeFailover.dita" xtrc="p:53;214:25">If
        successful this indicates the node is failed over.</p><p class="- topic/p " xtrf="/Users/Ruth/forks/docs-ng/dita/src/ag3/Tasks/tasks-nodeFailover.dita" xtrc="p:54;215:66">After you specify that a node is
        failed over you should handle the cause of failure and get your cluster back to a fully
        functional state. </p>
    </section>
    <section class="- topic/section " xtrf="/Users/Ruth/forks/docs-ng/dita/src/ag3/Tasks/tasks-nodeFailover.dita" xtrc="section:4;219:14"><title class="- topic/title " xtrf="/Users/Ruth/forks/docs-ng/dita/src/ag3/Tasks/tasks-nodeFailover.dita" xtrc="title:5;219:21">Handling a failover situation</title><p class="- topic/p " xtrf="/Users/Ruth/forks/docs-ng/dita/src/ag3/Tasks/tasks-nodeFailover.dita" xtrc="p:55;219:61">Any time that you automatically or
        manually failover a node, the cluster capacity will be reduced. Once a node is failed
        over:</p><ul class="- topic/ul " xtrf="/Users/Ruth/forks/docs-ng/dita/src/ag3/Tasks/tasks-nodeFailover.dita" xtrc="ul:14;221:22">
        <li class="- topic/li " xtrf="/Users/Ruth/forks/docs-ng/dita/src/ag3/Tasks/tasks-nodeFailover.dita" xtrc="li:22;222:13"><p class="- topic/p " xtrf="/Users/Ruth/forks/docs-ng/dita/src/ag3/Tasks/tasks-nodeFailover.dita" xtrc="p:56;222:16">The number of available nodes for each data bucket in your cluster will be reduced by
            one.</p></li>
        <li class="- topic/li " xtrf="/Users/Ruth/forks/docs-ng/dita/src/ag3/Tasks/tasks-nodeFailover.dita" xtrc="li:23;224:13"><p class="- topic/p " xtrf="/Users/Ruth/forks/docs-ng/dita/src/ag3/Tasks/tasks-nodeFailover.dita" xtrc="p:57;224:16">Replicated data handled by the failover node will be enabled on other nodes in the
            cluster.</p></li>
        <li class="- topic/li " xtrf="/Users/Ruth/forks/docs-ng/dita/src/ag3/Tasks/tasks-nodeFailover.dita" xtrc="li:24;226:13"><p class="- topic/p " xtrf="/Users/Ruth/forks/docs-ng/dita/src/ag3/Tasks/tasks-nodeFailover.dita" xtrc="p:58;226:16">Remaining nodes will have to handle all incoming requests for data.</p></li>
      </ul><p class="- topic/p " xtrf="/Users/Ruth/forks/docs-ng/dita/src/ag3/Tasks/tasks-nodeFailover.dita" xtrc="p:59;227:15">After a node has been failed over, you should perform a rebalance operation. The
        rebalance operation will:</p><ul class="- topic/ul " xtrf="/Users/Ruth/forks/docs-ng/dita/src/ag3/Tasks/tasks-nodeFailover.dita" xtrc="ul:15;228:42">
        <li class="- topic/li " xtrf="/Users/Ruth/forks/docs-ng/dita/src/ag3/Tasks/tasks-nodeFailover.dita" xtrc="li:25;229:13"><p class="- topic/p " xtrf="/Users/Ruth/forks/docs-ng/dita/src/ag3/Tasks/tasks-nodeFailover.dita" xtrc="p:60;229:16">Redistribute stored data across the remaining nodes within the cluster.</p></li>
        <li class="- topic/li " xtrf="/Users/Ruth/forks/docs-ng/dita/src/ag3/Tasks/tasks-nodeFailover.dita" xtrc="li:26;230:13"><p class="- topic/p " xtrf="/Users/Ruth/forks/docs-ng/dita/src/ag3/Tasks/tasks-nodeFailover.dita" xtrc="p:61;230:16">Recreate replicated data for all buckets at remaining nodes.</p></li>
        <li class="- topic/li " xtrf="/Users/Ruth/forks/docs-ng/dita/src/ag3/Tasks/tasks-nodeFailover.dita" xtrc="li:27;231:13"><p class="- topic/p " xtrf="/Users/Ruth/forks/docs-ng/dita/src/ag3/Tasks/tasks-nodeFailover.dita" xtrc="p:62;231:16">Return your cluster to the configured operational state.</p></li>
      </ul><p class="- topic/p " xtrf="/Users/Ruth/forks/docs-ng/dita/src/ag3/Tasks/tasks-nodeFailover.dita" xtrc="p:63;232:15">You may decide to add one or more new nodes to the cluster after a failover to return
        the cluster to a fully functional state. Better yet you may choose to replace the failed
        node and add additional nodes to provide more capacity than before. </p>
    </section>
    <section class="- topic/section " xtrf="/Users/Ruth/forks/docs-ng/dita/src/ag3/Tasks/tasks-nodeFailover.dita" xtrc="section:5;236:14"><title class="- topic/title " xtrf="/Users/Ruth/forks/docs-ng/dita/src/ag3/Tasks/tasks-nodeFailover.dita" xtrc="title:6;236:21">Adding back a failed over node</title><p class="- topic/p " xtrf="/Users/Ruth/forks/docs-ng/dita/src/ag3/Tasks/tasks-nodeFailover.dita" xtrc="p:64;236:62">You can add a failed over node back to
        the cluster if you identify and fix the issue that caused node failure. After Couchbase
        Server marks a node as failed over, the data on disk at the node will remain. A failed over
        node will not longer be <i class="+ topic/ph hi-d/i " xtrf="/Users/Ruth/forks/docs-ng/dita/src/ag3/Tasks/tasks-nodeFailover.dita" xtrc="i:4;239:36">synchronized</i> with the rest of the cluster; this means the
        node will no longer handle data request or receive replicated data.</p><p class="- topic/p " xtrf="/Users/Ruth/forks/docs-ng/dita/src/ag3/Tasks/tasks-nodeFailover.dita" xtrc="p:65;240:83">When you add a
        failed over node back into a cluster, the cluster will treat it as if it is a new node. This
        means that you should rebalance after you add the node to the cluster. This also means that
        any data stored on disk at that node will be destroyed when you perform this
          rebalance.</p><p class="- topic/p " xtrf="/Users/Ruth/forks/docs-ng/dita/src/ag3/Tasks/tasks-nodeFailover.dita" xtrc="p:66;244:28"><b class="+ topic/ph hi-d/b " xtrf="/Users/Ruth/forks/docs-ng/dita/src/ag3/Tasks/tasks-nodeFailover.dita" xtrc="b:18;244:31">Copy or Delete Data Files before Rejoining
        Cluster</b></p><p class="- topic/p " xtrf="/Users/Ruth/forks/docs-ng/dita/src/ag3/Tasks/tasks-nodeFailover.dita" xtrc="p:67;245:27">Therefore, before you add a failed over node back to the cluster, it is
        best practice to move or delete the persisted data files before you add the node back into
        the cluster. If you want to keep the files you can copy or move the files to another
        location such as another disk or EBS volume. When you add a node back into the cluster and
        then rebalance, data files will be deleted, recreated and repopulated.</p></section>
  </body>
  <related-links class="- topic/related-links " xtrf="/Users/Ruth/forks/docs-ng/dita/src/ag3/Tasks/tasks-nodeFailover.dita" xtrc="related-links:1;251:18"><linkpool class="- topic/linkpool " xtrc="topicref:44;53:51" xtrf="/Users/Ruth/forks/docs-ng/dita/src/ag3/Admin.ditamap"><link class="- topic/link " mapclass="- map/topicref " type="topic" xtrc="topicref:34;42:43" xtrf="/Users/Ruth/forks/docs-ng/dita/src/ag3/Admin.ditamap" href="../Tasks/tasks-common.dita" role="parent"><?ditaot usertext?><linktext class="- topic/linktext "><?ditaot gentext?>Common administrative tasks</linktext></link></linkpool>
    <linklist class="- topic/linklist " xtrf="/Users/Ruth/forks/docs-ng/dita/src/ag3/Tasks/tasks-nodeFailover.dita" xtrc="linklist:1;252:15">
      <link href="tasks-rebalance.dita" class="- topic/link " xtrf="/Users/Ruth/forks/docs-ng/dita/src/ag3/Tasks/tasks-nodeFailover.dita" xtrc="link:1;253:51" type="topic"><?ditaot gentext?><linktext class="- topic/linktext ">Rebalancing</linktext></link>
      <link href="../UI/ui-settings.dita" class="- topic/link " xtrf="/Users/Ruth/forks/docs-ng/dita/src/ag3/Tasks/tasks-nodeFailover.dita" xtrc="link:2;254:44" type="topic"><?ditaot gentext?><linktext class="- topic/linktext ">Settings</linktext><?ditaot genshortdesc?><desc class="- topic/desc ">The <uicontrol class="+ topic/ph ui-d/uicontrol " xtrf="/Users/Ruth/forks/docs-ng/dita/src/ag3/UI/ui-settings.dita" xtrc="uicontrol:1;6:29">Settings</uicontrol> section provides configuration and information      for the cluster, update notifications, auto failover, alerts, auto compaction, sample buckets, and account management.</desc></link>
      <link href="../REST/rest-cluster-autofailover-intro.dita" class="- topic/link " xtrf="/Users/Ruth/forks/docs-ng/dita/src/ag3/Tasks/tasks-nodeFailover.dita" xtrc="link:3;255:66" type="topic"><?ditaot gentext?><linktext class="- topic/linktext ">Managing auto-failover</linktext></link>
      <link href="../REST/rest-cluster-email-notifications.dita" class="- topic/link " xtrf="/Users/Ruth/forks/docs-ng/dita/src/ag3/Tasks/tasks-nodeFailover.dita" xtrc="link:4;256:67" type="topic"><?ditaot gentext?><linktext class="- topic/linktext ">View settings for email notifications</linktext></link>
    </linklist>
  </related-links>
</topic>